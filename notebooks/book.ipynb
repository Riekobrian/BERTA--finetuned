{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [],
   "source": [
    "data=pd.read_csv(\"C:/Users/Ricky/Desktop/MASTERS/DATA SCIENCE for Decision Making-MILE/Sentiment Analysis/cleaned_reviews.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [],
   "source": [
    "shapes = pd.read_csv('cleaned_reviews.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      sentiments                                     cleaned_review  \\\n",
      "0       positive  i wish would have gotten one earlier love it a...   \n",
      "1        neutral  i ve learned this lesson again open the packag...   \n",
      "2        neutral          it is so slow and lags find better option   \n",
      "3        neutral  roller ball stopped working within months of m...   \n",
      "4        neutral  i like the color and size but it few days out ...   \n",
      "...          ...                                                ...   \n",
      "17335   positive  i love this speaker and love can take it anywh...   \n",
      "17336   positive  i use it in my house easy to connect and loud ...   \n",
      "17337   positive  the bass is good and the battery is amazing mu...   \n",
      "17338   positive                                            love it   \n",
      "17339    neutral                                       mono speaker   \n",
      "\n",
      "       cleaned_review_length  review_score  \n",
      "0                         19           5.0  \n",
      "1                         88           1.0  \n",
      "2                          9           2.0  \n",
      "3                         12           1.0  \n",
      "4                         21           1.0  \n",
      "...                      ...           ...  \n",
      "17335                     30           5.0  \n",
      "17336                     13           4.0  \n",
      "17337                     41           5.0  \n",
      "17338                      2           5.0  \n",
      "17339                      2           5.0  \n",
      "\n",
      "[17340 rows x 4 columns]\n"
     ]
    }
   ],
   "source": [
    "print(shapes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 17340 entries, 0 to 17339\n",
      "Data columns (total 4 columns):\n",
      " #   Column                 Non-Null Count  Dtype  \n",
      "---  ------                 --------------  -----  \n",
      " 0   sentiments             17340 non-null  object \n",
      " 1   cleaned_review         17337 non-null  object \n",
      " 2   cleaned_review_length  17340 non-null  int64  \n",
      " 3   review_score           17340 non-null  float64\n",
      "dtypes: float64(1), int64(1), object(2)\n",
      "memory usage: 542.0+ KB\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "print(data.info())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Column names in the CSV file:'\n",
      "Index(['sentiments', 'cleaned_review', 'cleaned_review_length',\n",
      "       'review_score'],\n",
      "      dtype='object')\n"
     ]
    }
   ],
   "source": [
    "print(\"Column names in the CSV file:'\")\n",
    "print(data.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "       cleaned_review_length  review_score\n",
      "count           17340.000000  17340.000000\n",
      "mean               30.300461      3.649077\n",
      "std                35.836540      1.673500\n",
      "min                 0.000000      1.000000\n",
      "25%                 9.000000      2.000000\n",
      "50%                20.000000      5.000000\n",
      "75%                38.000000      5.000000\n",
      "max               571.000000      5.000000\n"
     ]
    }
   ],
   "source": [
    "print(data.describe())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import seaborn\n",
    "sns.histplot(data['sentiments'])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Importing the necessary libraries and Modules**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "import tensorflow as tf\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Getting started with NLTK Resources - DOWNLOADING**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 115,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('punkt', quiet=True)\n",
    "nltk.download('stopwords', quiet=True)\n",
    "nltk.download('wordnet', quiet=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Preprocessing class**\n",
    " - - To handle the preprocessinglogic in an organized way"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SentimentDataPreprocessor:\n",
    "    def __init__(self, data_path):\n",
    "        \"\"\"\n",
    "        Initialize the preprocessing class.\n",
    "        \"\"\"\n",
    "        nltk.download('punkt', quiet=True)\n",
    "        nltk.download('stopwords', quiet=True)\n",
    "        nltk.download('wordnet', quiet=True)\n",
    "\n",
    "        self.lemmatizer = WordNetLemmatizer()\n",
    "        self.stop_words = set(stopwords.words('english'))\n",
    "        self.raw_data = pd.read_csv(data_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Defining the clean_text Method\n",
    "- - Cleaning pipeline to prepare the data for modeling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_text(self, text):\n",
    "        \"\"\"\n",
    "        Clean and preprocess a single text entry.\n",
    "        \"\"\"\n",
    "        # Convert to lowercase\n",
    "        text = text.lower()\n",
    "\n",
    "        # Tokenize\n",
    "        tokens = nltk.word_tokenize(text)\n",
    "\n",
    "        # Remove non-alphanumeric tokens and stop words, then lemmatize\n",
    "        cleaned_tokens = [\n",
    "            self.lemmatizer.lemmatize(token) \n",
    "            for token in tokens \n",
    "            if token.isalnum() and token not in self.stop_words\n",
    "        ]\n",
    "        \n",
    "        return ' '.join(cleaned_tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Preprocess_data -- Defining the method\n",
    "- - Clen text data, encode labels and spli the dataset for training and testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_data(self, text_column, label_column):\n",
    "        \"\"\"\n",
    "        Preprocess text data and split into training and testing sets.\n",
    "        \"\"\"\n",
    "        # Preprocess the text column\n",
    "        self.raw_data['processed_text'] = self.raw_data[text_column].apply(self.preprocess_text)\n",
    "\n",
    "        # Split data into train and test sets\n",
    "        X_train, X_test, y_train, y_test = train_test_split(\n",
    "            self.raw_data['processed_text'], \n",
    "            self.raw_data[label_column], \n",
    "            test_size=0.2, \n",
    "            random_state=42\n",
    "        )\n",
    "        \n",
    "        return {\n",
    "            'train': (X_train, y_train),\n",
    "            'test': (X_test, y_test)\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_data(self, text_column, label_column):\n",
    "        \"\"\"\n",
    "        Preprocess text data and split into training and testing sets.\n",
    "        \"\"\"\n",
    "        # Clean the text data\n",
    "        self.raw_data['cleaned_text'] = self.raw_data[text_column].apply(self.clean_text)\n",
    "        \n",
    "        # Encode labels\n",
    "        label_encoder = LabelEncoder()\n",
    "        self.raw_data['encoded_label'] = label_encoder.fit_transform(self.raw_data[label_column])\n",
    "        \n",
    "        # Split data into train and test sets\n",
    "        X_train, X_test, y_train, y_test = train_test_split(\n",
    "            self.raw_data['cleaned_text'], \n",
    "            self.raw_data['encoded_label'], \n",
    "            test_size=0.2, \n",
    "            random_state=42\n",
    "        )\n",
    "        \n",
    "        return {\n",
    "            'train': (X_train, y_train),\n",
    "            'test': (X_test, y_test),\n",
    "            'label_encoder': label_encoder\n",
    "        }\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Creating the tokenizer method\n",
    "- - Fit a tokenozer to text data for use with DPL models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_tokenizer(self, texts, max_words=5000):\n",
    "        \"\"\"\n",
    "        Create a tokenizer fitted to the text data.\n",
    "        \"\"\"\n",
    "        tokenizer = tf.keras.preprocessing.text.Tokenizer(\n",
    "            num_words=max_words, \n",
    "            oov_token='<OOV>'\n",
    "        )\n",
    "        tokenizer.fit_on_texts(texts)\n",
    "        return tokenizer\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sequence pad method\n",
    "- - Convert text data into padded sequence suitable for DL models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sequence_pad(self, tokenizer, texts, max_length=100):\n",
    "        \"\"\"\n",
    "        Convert text data into padded sequences.\n",
    "        \"\"\"\n",
    "        sequences = tokenizer.texts_to_sequences(texts)\n",
    "        padded_sequences = tf.keras.preprocessing.sequence.pad_sequences(\n",
    "            sequences, \n",
    "            maxlen=max_length, \n",
    "            padding='post', \n",
    "            truncating='post'\n",
    "        )\n",
    "        return padded_sequences\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Instantiate the preprocessor\n",
    "-  - Instantiate the preprocessor class and load the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_path =  './cleaned_reviews.csv'\n",
    "preprocessor  = SentimentDataPreprocessor(data_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assuming the cleaned dataset has 'reviews_text' and 'sentiments' columns\n",
    "processed = preprocessor.preprocess_data(text_column='reviews_text', label_column='sentiments')\n",
    "\n",
    "# Train and test splits\n",
    "X_train, y_train = processed['train']\n",
    "X_test, y_test = processed['test']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_path = './cleaned_reviews.csv'\n",
    "preprocessor = SentimentDataPreprocessor(data_path)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Apply text cleaning to the data:encode labels and split the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "class SentimentDataPreprocessor:\n",
    "    def __init__(self, data_path):\n",
    "        \"\"\"\n",
    "        Initialize the preprocessing class.\n",
    "        \"\"\"\n",
    "        nltk.download('punkt', quiet=True)\n",
    "        nltk.download('stopwords', quiet=True)\n",
    "        nltk.download('wordnet', quiet=True)\n",
    "\n",
    "        self.lemmatizer = WordNetLemmatizer()\n",
    "        self.stop_words = set(stopwords.words('english'))\n",
    "        self.raw_data = pd.read_csv(data_path)\n",
    "\n",
    "    def preprocess_text(self, text):\n",
    "        \"\"\"\n",
    "        Clean and preprocess a single text entry.\n",
    "        \"\"\"\n",
    "        # Convert to lowercase\n",
    "        text = text.lower()\n",
    "\n",
    "        # Tokenize\n",
    "        tokens = word_tokenize(text)\n",
    "\n",
    "        # Remove non-alphanumeric tokens and stop words, then lemmatize\n",
    "        cleaned_tokens = [\n",
    "            self.lemmatizer.lemmatize(token) \n",
    "            for token in tokens \n",
    "            if token.isalnum() and token not in self.stop_words\n",
    "        ]\n",
    "        \n",
    "        return ' '.join(cleaned_tokens)\n",
    "\n",
    "    def preprocess_data(self, text_column, label_column):\n",
    "        \"\"\"\n",
    "        Preprocess text data and split into training and testing sets.\n",
    "        \"\"\"\n",
    "        # Preprocess the text column\n",
    "        self.raw_data['processed_text'] = self.raw_data[text_column].apply(self.preprocess_text)\n",
    "\n",
    "        # Split data into train and test sets\n",
    "        X_train, X_test, y_train, y_test = train_test_split(\n",
    "            self.raw_data['processed_text'], \n",
    "            self.raw_data[label_column], \n",
    "            test_size=0.2, \n",
    "            random_state=42\n",
    "        )\n",
    "        \n",
    "        return {\n",
    "            'train': (X_train, y_train),\n",
    "            'test': (X_test, y_test)\n",
    "        }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_path = './cleaned_reviews.csv'\n",
    "preprocessor = SentimentDataPreprocessor(data_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_text(self, text):\n",
    "    \"\"\"\n",
    "    Clean and preprocess a single text entry.\n",
    "    \"\"\"\n",
    "    if not isinstance(text, str):\n",
    "        # Return an empty string if the input is not a valid string\n",
    "        return \"\"\n",
    "\n",
    "    # Convert to lowercase\n",
    "    text = text.lower()\n",
    "\n",
    "    # Tokenize\n",
    "    tokens = word_tokenize(text)\n",
    "\n",
    "    # Remove non-alphanumeric tokens and stop words, then lemmatize\n",
    "    cleaned_tokens = [\n",
    "        self.lemmatizer.lemmatize(token)\n",
    "        for token in tokens\n",
    "        if token.isalnum() and token not in self.stop_words\n",
    "    ]\n",
    "    \n",
    "    return ' '.join(cleaned_tokens)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_data(self, text_column, label_column):\n",
    "    \"\"\"\n",
    "    Preprocess text data and split into training and testing sets.\n",
    "    \"\"\"\n",
    "    # Convert all text entries to strings to handle missing values or non-string types\n",
    "    self.raw_data[text_column] = self.raw_data[text_column].astype(str)\n",
    "\n",
    "    # Preprocess the text column\n",
    "    self.raw_data['processed_text'] = self.raw_data[text_column].apply(self.preprocess_text)\n",
    "\n",
    "    # Split data into train and test sets\n",
    "    X_train, X_test, y_train, y_test = train_test_split(\n",
    "        self.raw_data['processed_text'], \n",
    "        self.raw_data[label_column], \n",
    "        test_size=0.2, \n",
    "        random_state=42\n",
    "    )\n",
    "    \n",
    "    return {\n",
    "        'train': (X_train, y_train),\n",
    "        'test': (X_test, y_test)\n",
    "    }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "processed = preprocessor.preprocess_data(text_column='cleaned_review', label_column='sentiments')\n",
    "X_train, y_train = processed['train']\n",
    "X_test, y_test = processed['test']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preprocess the dataset\n",
    "processed = preprocessor.preprocess_data(text_column='reviews_text', label_column='sentiments')\n",
    "\n",
    "# Access train and test splits\n",
    "X_train, y_train = processed['train']\n",
    "X_test, y_test = processed['test']"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
